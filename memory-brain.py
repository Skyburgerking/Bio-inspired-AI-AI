# memory_system.py - æ™ºèƒ½è®°å¿†ç³»ç»Ÿï¼ˆå¸¦å‹ç¼©å­˜å‚¨å’Œæ™ºèƒ½å¢å¼ºï¼‰- ä¿®å¤ç‰ˆ
import json
import pickle
import gzip
import zipfile
import lz4.frame
import hashlib
import base64
import msgpack
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union, Tuple
import numpy as np
from collections import defaultdict, deque
import heapq
import re
import math
import traceback

# å°è¯•å¯¼å…¥ zstandardï¼Œå¦‚æœå¤±è´¥åˆ™æä¾›ä¸€ä¸ªæ›¿ä»£æ–¹æ¡ˆ
try:
    import zstandard as zstd
    ZSTD_AVAILABLE = True
except ImportError:
    ZSTD_AVAILABLE = False
    print("âš ï¸ zstandard åº“æœªå®‰è£…ï¼Œzstd å‹ç¼©ç®—æ³•å°†ä¸å¯ç”¨")

class CompressedMemory:
    """å‹ç¼©è®°å¿†å­˜å‚¨ï¼ˆå¢å¼ºç‰ˆï¼‰- ä¿®å¤ç‰ˆ"""
    
    COMPRESSION_ALGORITHMS = {
        'gzip': {
            'compress': lambda data: gzip.compress(data, compresslevel=6),
            'decompress': gzip.decompress,
            'extension': '.gz'
        },
        'lz4': {
            'compress': lambda data: lz4.frame.compress(data),
            'decompress': lz4.frame.decompress,
            'extension': '.lz4'
        },
        'none': {
            'compress': lambda data: data,
            'decompress': lambda data: data,
            'extension': ''
        }
    }
    
    def __init__(self, config):
        self.config = config
        self.memory_dir = config.get_path("memory_dir")
        self.compression_config = config.get("compression", {})
        self.algorithm = self.compression_config.get("algorithm", "gzip")
        
        # æ·»åŠ  zstd ç®—æ³•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if ZSTD_AVAILABLE:
            self.COMPRESSION_ALGORITHMS['zstd'] = {
                'compress': lambda data: zstd.compress(data),
                'decompress': zstd.decompress,
                'extension': '.zst'
            }
        
        if self.algorithm not in self.COMPRESSION_ALGORITHMS:
            self.algorithm = "gzip"
        
        # å†…å­˜ç¼“å­˜ï¼ˆå¢å¼ºï¼‰
        self.memory_cache = {}
        self.cache_size_limit = 2000
        self.access_counter = defaultdict(int)
        self.access_timestamps = {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_compressed": 0,
            "total_decompressed": 0,
            "compression_ratio": 1.0,
            "saved_space_mb": 0.0,
            "cache_hits": 0,
            "cache_misses": 0,
            "avg_access_time": 0,
            "failed_decompressions": 0,
            "corrupted_files": set()  # è®°å½•æŸåçš„æ–‡ä»¶
        }
        
        # åˆ›å»ºè®°å¿†ç›®å½•
        self.memory_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ—¶æ£€æŸ¥å¹¶ä¿®å¤æŸåçš„æ–‡ä»¶
        self.check_and_repair_files()
    
    def get_optimal_algorithm(self, data_size: int, importance: float) -> str:
        """æ ¹æ®æ•°æ®å¤§å°å’Œé‡è¦æ€§é€‰æ‹©æœ€ä½³å‹ç¼©ç®—æ³•"""
        if importance > 0.8:  # é«˜é‡è¦æ€§æ•°æ®
            return 'gzip'  # gzipæ›´å¯é 
        elif ZSTD_AVAILABLE and data_size > 1024 * 1024:  # å¤§äº1MB
            return 'zstd'  # zstdå‹ç¼©æ¯”é«˜
        elif data_size < 1024:  # å°äº1KB
            return 'lz4'  # lz4é€Ÿåº¦å¿«
        else:
            return self.algorithm
    
    def compress_data(self, data: Any, importance: float = 0.5) -> Tuple[bytes, str]:
        """å‹ç¼©æ•°æ®ï¼ˆæ™ºèƒ½é€‰æ‹©ç®—æ³•ï¼‰"""
        try:
            # åºåˆ—åŒ–æ•°æ®
            serialized = msgpack.packb(data, use_bin_type=True)
            
            # æ ¹æ®æ•°æ®å¤§å°å’Œé‡è¦æ€§é€‰æ‹©ç®—æ³•
            data_size = len(serialized)
            optimal_algo = self.get_optimal_algorithm(data_size, importance)
            
            # æ£€æŸ¥ç®—æ³•æ˜¯å¦å¯ç”¨
            if optimal_algo not in self.COMPRESSION_ALGORITHMS:
                optimal_algo = self.algorithm
            
            # åº”ç”¨å‹ç¼©
            if optimal_algo != "none":
                compressed = self.COMPRESSION_ALGORITHMS[optimal_algo]['compress'](serialized)
                
                # æ›´æ–°ç»Ÿè®¡
                original_size = len(serialized)
                compressed_size = len(compressed)
                ratio = compressed_size / original_size if original_size > 0 else 1.0
                
                self.stats["total_compressed"] += 1
                self.stats["compression_ratio"] = 0.9 * self.stats["compression_ratio"] + 0.1 * ratio
                self.stats["saved_space_mb"] += (original_size - compressed_size) / (1024 * 1024)
                
                return compressed, optimal_algo
            else:
                return serialized, "none"
                
        except Exception as e:
            print(f"å‹ç¼©å¤±è´¥: {e}")
            return pickle.dumps(data), "pickle"
    
    def decompress_data(self, compressed_data: bytes, algorithm: str = None) -> Any:
        """è§£å‹æ•°æ®ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        start_time = datetime.now()
        
        try:
            # å¦‚æœæŒ‡å®šäº†ç®—æ³•ä¸”å¯ç”¨
            if algorithm and algorithm in self.COMPRESSION_ALGORITHMS:
                try:
                    decompressed = self.COMPRESSION_ALGORITHMS[algorithm]['decompress'](compressed_data)
                    data = msgpack.unpackb(decompressed, raw=False)
                    
                    # è®¡ç®—è®¿é—®æ—¶é—´
                    access_time = (datetime.now() - start_time).total_seconds() * 1000
                    self.stats["avg_access_time"] = 0.9 * self.stats["avg_access_time"] + 0.1 * access_time
                    self.stats["total_decompressed"] += 1
                    
                    return data
                    
                except Exception as e:
                    print(f"âš ï¸ ä½¿ç”¨ {algorithm} è§£å‹å¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹: {e}")
            
            # è‡ªåŠ¨æ£€æµ‹ç®—æ³•
            algorithms_to_try = []
            
            # å¦‚æœæŒ‡å®šäº†ç®—æ³•ä½†ä¸æ˜¯ç¬¬ä¸€ä¸ªï¼Œå…ˆå°è¯•æŒ‡å®šçš„
            if algorithm and algorithm != "none":
                algorithms_to_try.append(algorithm)
            
            # æ·»åŠ æ‰€æœ‰å¯ç”¨ç®—æ³•
            for algo in ['gzip', 'lz4', 'zstd', 'none']:
                if algo in self.COMPRESSION_ALGORITHMS and algo != algorithm:
                    algorithms_to_try.append(algo)
            
            # å°è¯• pickle ä½œä¸ºæœ€åæ‰‹æ®µ
            algorithms_to_try.append('pickle')
            
            # å°è¯•æ¯ä¸ªç®—æ³•
            for algo in algorithms_to_try:
                try:
                    if algo == "none":
                        data = msgpack.unpackb(compressed_data, raw=False)
                    elif algo == "pickle":
                        data = pickle.loads(compressed_data)
                    else:
                        decompressed = self.COMPRESSION_ALGORITHMS[algo]['decompress'](compressed_data)
                        data = msgpack.unpackb(decompressed, raw=False)
                    
                    # è®¡ç®—è®¿é—®æ—¶é—´
                    access_time = (datetime.now() - start_time).total_seconds() * 1000
                    self.stats["avg_access_time"] = 0.9 * self.stats["avg_access_time"] + 0.1 * access_time
                    self.stats["total_decompressed"] += 1
                    
                    print(f"âœ… ä½¿ç”¨ {algo} ç®—æ³•æˆåŠŸè§£å‹æ•°æ®")
                    return data
                    
                except Exception as e:
                    continue
            
            # æ‰€æœ‰ç®—æ³•éƒ½å¤±è´¥
            self.stats["failed_decompressions"] += 1
            print(f"âŒ æ‰€æœ‰è§£å‹æ–¹æ³•éƒ½å¤±è´¥")
            return None
            
        except Exception as e:
            self.stats["failed_decompressions"] += 1
            print(f"è§£å‹å¤±è´¥: {e}")
            return None
    
    def check_and_repair_files(self):
        """æ£€æŸ¥å¹¶ä¿®å¤æŸåçš„æ–‡ä»¶"""
        print("ğŸ”„ æ£€æŸ¥è®°å¿†æ–‡ä»¶å®Œæ•´æ€§...")
        
        repaired = 0
        corrupted = 0
        
        for filepath in self.memory_dir.glob("*"):
            if filepath.suffix in ['.meta', '.json']:
                continue
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            if filepath.stat().st_size == 0:
                print(f"âš ï¸ å‘ç°ç©ºæ–‡ä»¶: {filepath.name}")
                corrupted += 1
                self.stats["corrupted_files"].add(str(filepath))
                continue
            
            # å°è¯•è¯»å–æ–‡ä»¶
            try:
                with open(filepath, 'rb') as f:
                    compressed = f.read()
                
                # æŸ¥æ‰¾å¯¹åº”çš„å…ƒæ•°æ®æ–‡ä»¶
                meta_file = filepath.with_suffix('.meta')
                algorithm = self.algorithm
                
                if meta_file.exists():
                    try:
                        with open(meta_file, 'r', encoding='utf-8') as f:
                            meta_info = json.load(f)
                            algorithm = meta_info.get("algorithm", self.algorithm)
                    except:
                        algorithm = self.algorithm
                
                # å°è¯•è§£å‹
                data = self.decompress_data(compressed, algorithm)
                
                if data is None:
                    print(f"âš ï¸ æ–‡ä»¶å¯èƒ½å·²æŸå: {filepath.name}")
                    corrupted += 1
                    self.stats["corrupted_files"].add(str(filepath))
                else:
                    repaired += 1
                    
            except Exception as e:
                print(f"âŒ æ£€æŸ¥æ–‡ä»¶ {filepath.name} æ—¶å‡ºé”™: {e}")
                corrupted += 1
                self.stats["corrupted_files"].add(str(filepath))
        
        print(f"âœ… æ–‡ä»¶æ£€æŸ¥å®Œæˆ: {repaired} ä¸ªæ­£å¸¸, {corrupted} ä¸ªå¯èƒ½æŸå")
        return repaired, corrupted
    
    def save_memory(self, memory_id: str, data: Any, metadata: Dict = None) -> bool:
        """ä¿å­˜è®°å¿†ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼‰"""
        try:
            # å‡†å¤‡æ•°æ®
            memory_data = {
                "data": data,
                "metadata": metadata or {},
                "created_at": datetime.now().isoformat(),
                "version": "1.2",  # ç‰ˆæœ¬æ›´æ–°
                "access_count": 0,
                "last_accessed": None
            }
            
            # è·å–é‡è¦æ€§
            importance = metadata.get("importance", 0.5) if metadata else 0.5
            
            # å‹ç¼©ï¼ˆæ™ºèƒ½é€‰æ‹©ç®—æ³•ï¼‰
            compressed, algorithm_used = self.compress_data(memory_data, importance)
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_ext = self.COMPRESSION_ALGORITHMS.get(algorithm_used, {}).get('extension', '.bin')
            filename = f"{memory_id}_{timestamp}_{importance:.2f}{file_ext}"
            filepath = self.memory_dir / filename
            
            # ä¿å­˜æ–‡ä»¶ï¼ˆåŒ…å«ç®—æ³•ä¿¡æ¯ï¼‰
            with open(filepath, 'wb') as f:
                f.write(compressed)
            
            # ä¿å­˜ç®—æ³•ä¿¡æ¯åˆ°å…ƒæ–‡ä»¶
            meta_info = {
                "algorithm": algorithm_used,
                "size": len(compressed),
                "importance": importance,
                "memory_id": memory_id,
                "original_size": len(pickle.dumps(memory_data)),
                "compression_ratio": len(compressed) / max(1, len(pickle.dumps(memory_data))),
                "created_at": datetime.now().isoformat()
            }
            meta_file = filepath.with_suffix('.meta')
            with open(meta_file, 'w', encoding='utf-8') as f:
                json.dump(meta_info, f, ensure_ascii=False, indent=2)
            
            # æ›´æ–°ç¼“å­˜
            self.memory_cache[memory_id] = {
                "data": data,
                "metadata": metadata,
                "filepath": filepath,
                "accessed": datetime.now(),
                "access_count": 0,
                "importance": importance,
                "algorithm": algorithm_used,
                "compressed_size": len(compressed)
            }
            
            # æ™ºèƒ½ç¼“å­˜ç®¡ç†
            self.manage_cache()
            
            print(f"âœ… ä¿å­˜è®°å¿†: {memory_id} ({len(compressed)} å­—èŠ‚)")
            return True
            
        except Exception as e:
            print(f"âŒ ä¿å­˜è®°å¿†å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def load_memory(self, memory_id: str, pattern: str = None) -> Optional[Any]:
        """åŠ è½½è®°å¿†ï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰"""
        # æ£€æŸ¥ç¼“å­˜
        if memory_id in self.memory_cache:
            cache_entry = self.memory_cache[memory_id]
            cache_entry["accessed"] = datetime.now()
            cache_entry["access_count"] += 1
            self.access_counter[memory_id] += 1
            self.stats["cache_hits"] += 1
            return cache_entry["data"]
        
        self.stats["cache_misses"] += 1
        
        # æŸ¥æ‰¾æ–‡ä»¶
        files = []
        if pattern:
            files = list(self.memory_dir.glob(f"{pattern}*"))
        else:
            files = list(self.memory_dir.glob(f"{memory_id}_*"))
        
        if not files:
            print(f"âŒ æœªæ‰¾åˆ°è®°å¿†æ–‡ä»¶: {memory_id}")
            return None
        
        # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„ä¼˜å…ˆï¼‰ï¼ŒåŒæ—¶è€ƒè™‘é‡è¦æ€§
        def file_score(filepath):
            # ä»æ–‡ä»¶åæå–é‡è¦æ€§
            try:
                importance_match = re.search(r'_(\d+\.\d{2})', filepath.name)
                importance = float(importance_match.group(1)) if importance_match else 0.5
            except:
                importance = 0.5
            
            # æ£€æŸ¥æ˜¯å¦åœ¨æŸåæ–‡ä»¶åˆ—è¡¨ä¸­
            if str(filepath) in self.stats["corrupted_files"]:
                return -1000  # æŸåæ–‡ä»¶å¾—åˆ†å¾ˆä½
            
            mtime = filepath.stat().st_mtime
            # é‡è¦æ€§é«˜çš„æ–‡ä»¶ä¼˜å…ˆï¼Œæ—¶é—´æ–°çš„ä¼˜å…ˆ
            return importance * 1000 + mtime
        
        files.sort(key=file_score, reverse=True)
        
        for filepath in files:
            try:
                # æ£€æŸ¥æ˜¯å¦åœ¨æŸåæ–‡ä»¶åˆ—è¡¨ä¸­
                if str(filepath) in self.stats["corrupted_files"]:
                    print(f"âš ï¸ è·³è¿‡å·²çŸ¥æŸåæ–‡ä»¶: {filepath.name}")
                    continue
                
                # è¯»å–ç®—æ³•ä¿¡æ¯
                meta_file = filepath.with_suffix('.meta')
                algorithm = self.algorithm
                importance = 0.5
                
                if meta_file.exists():
                    try:
                        with open(meta_file, 'r', encoding='utf-8') as f:
                            meta_info = json.load(f)
                            algorithm = meta_info.get("algorithm", self.algorithm)
                            importance = meta_info.get("importance", 0.5)
                    except:
                        algorithm = self.algorithm
                        importance = 0.5
                
                # è¯»å–æ–‡ä»¶
                with open(filepath, 'rb') as f:
                    compressed = f.read()
                
                # è§£å‹
                memory_data = self.decompress_data(compressed, algorithm)
                
                if memory_data:
                    # æ›´æ–°è®¿é—®ç»Ÿè®¡
                    memory_data["access_count"] = memory_data.get("access_count", 0) + 1
                    memory_data["last_accessed"] = datetime.now().isoformat()
                    
                    # æ›´æ–°ç¼“å­˜
                    self.memory_cache[memory_id] = {
                        "data": memory_data["data"],
                        "metadata": memory_data.get("metadata", {}),
                        "filepath": filepath,
                        "accessed": datetime.now(),
                        "access_count": memory_data["access_count"],
                        "importance": importance,
                        "algorithm": algorithm,
                        "compressed_size": len(compressed)
                    }
                    
                    self.access_counter[memory_id] = memory_data["access_count"]
                    
                    # å¼‚æ­¥æ›´æ–°æ–‡ä»¶ä¸­çš„è®¿é—®ç»Ÿè®¡
                    self.update_access_stats_async(filepath, memory_data["access_count"])
                    
                    print(f"âœ… åŠ è½½è®°å¿†: {memory_id} ({len(compressed)} å­—èŠ‚)")
                    return memory_data["data"]
                else:
                    print(f"âš ï¸ è§£å‹å¤±è´¥ï¼Œæ ‡è®°ä¸ºæŸå: {filepath.name}")
                    self.stats["corrupted_files"].add(str(filepath))
            except Exception as e:
                print(f"âŒ åŠ è½½è®°å¿†å¤±è´¥ {filepath.name}: {e}")
                self.stats["corrupted_files"].add(str(filepath))
                continue
        
        return None
    
    def update_access_stats_async(self, filepath: Path, access_count: int):
        """å¼‚æ­¥æ›´æ–°è®¿é—®ç»Ÿè®¡"""
        try:
            meta_file = filepath.with_suffix('.meta')
            if meta_file.exists():
                with open(meta_file, 'r', encoding='utf-8') as f:
                    meta_info = json.load(f)
                
                meta_info["access_count"] = access_count
                meta_info["last_accessed"] = datetime.now().isoformat()
                
                with open(meta_file, 'w', encoding='utf-8') as f:
                    json.dump(meta_info, f, ensure_ascii=False, indent=2)
        except:
            pass
    
    def search_memories(self, query: str, limit: int = 10) -> List[Dict]:
        """æœç´¢è®°å¿†ï¼ˆæ™ºèƒ½æœç´¢ï¼‰"""
        results = []
        query_lower = query.lower()
        
        # 1. é¦–å…ˆæœç´¢ç¼“å­˜
        for memory_id, cache_entry in self.memory_cache.items():
            try:
                data_str = str(cache_entry["data"]).lower()
                metadata_str = str(cache_entry.get("metadata", {})).lower()
                
                # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
                score = self.calculate_relevance_score(query_lower, data_str, metadata_str, cache_entry)
                if score > 0:
                    results.append({
                        "file": cache_entry.get("filepath", Path()).name,
                        "data": cache_entry["data"],
                        "metadata": cache_entry.get("metadata", {}),
                        "created": cache_entry.get("metadata", {}).get("created_at", ""),
                        "relevance_score": score,
                        "source": "cache"
                    })
            except:
                continue
        
        # 2. æœç´¢æ–‡ä»¶ç³»ç»Ÿï¼ˆå¦‚æœç¼“å­˜ç»“æœä¸å¤Ÿï¼‰
        if len(results) < limit:
            for filepath in self.memory_dir.glob("*"):
                if filepath.suffix in ['.meta', '.json']:
                    continue
                    
                # è·³è¿‡æŸåæ–‡ä»¶
                if str(filepath) in self.stats["corrupted_files"]:
                    continue
                
                try:
                    # æ£€æŸ¥æ˜¯å¦å·²åœ¨ç¼“å­˜ç»“æœä¸­
                    if any(r["file"] == filepath.name for r in results):
                        continue
                        
                    # è¯»å–ç®—æ³•ä¿¡æ¯
                    algorithm = self.algorithm
                    meta_file = filepath.with_suffix('.meta')
                    if meta_file.exists():
                        with open(meta_file, 'r', encoding='utf-8') as f:
                            meta_info = json.load(f)
                            algorithm = meta_info.get("algorithm", self.algorithm)
                    
                    # è¯»å–æ–‡ä»¶
                    with open(filepath, 'rb') as f:
                        compressed = f.read()
                    
                    # è§£å‹å¹¶æœç´¢
                    memory_data = self.decompress_data(compressed, algorithm)
                    if memory_data:
                        data_str = str(memory_data).lower()
                        metadata = memory_data.get("metadata", {})
                        metadata_str = str(metadata).lower()
                        
                        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
                        cache_entry = {
                            "importance": meta_info.get("importance", 0.5) if meta_file.exists() else 0.5,
                            "access_count": meta_info.get("access_count", 0) if meta_file.exists() else 0
                        }
                        score = self.calculate_relevance_score(query_lower, data_str, metadata_str, cache_entry)
                        
                        if score > 0:
                            results.append({
                                "file": filepath.name,
                                "data": memory_data["data"],
                                "metadata": metadata,
                                "created": metadata.get("created_at", ""),
                                "relevance_score": score,
                                "source": "file"
                            })
                            
                            if len(results) >= limit * 2:  # å¤šæ”¶é›†ä¸€äº›ç”¨äºæ’åº
                                break
                except:
                    continue
        
        # 3. æŒ‰ç›¸å…³æ€§æ’åº
        results.sort(key=lambda x: x["relevance_score"], reverse=True)
        
        return results[:limit]
    
    def calculate_relevance_score(self, query: str, data_str: str, metadata_str: str, cache_entry: Dict) -> float:
        """è®¡ç®—ç›¸å…³æ€§åˆ†æ•°"""
        score = 0.0
        
        # 1. ç›´æ¥åŒ¹é…
        if query in data_str:
            score += 2.0
        if query in metadata_str:
            score += 1.5
        
        # 2. éƒ¨åˆ†åŒ¹é…
        words = query.split()
        for word in words:
            if len(word) > 2:
                if word in data_str:
                    score += 0.5
                if word in metadata_str:
                    score += 0.3
        
        # 3. é‡è¦æ€§æƒé‡
        importance = cache_entry.get("importance", 0.5)
        score *= (0.5 + importance)  # é‡è¦æ€§é«˜çš„è®°å¿†å¾—åˆ†æ›´é«˜
        
        # 4. è®¿é—®é¢‘ç‡æƒé‡
        access_count = cache_entry.get("access_count", 0)
        if access_count > 0:
            score *= (1.0 + math.log(1 + access_count) / 10)
        
        return score
    
    def cleanup_old_memories(self, days_old: int = 30, keep_important: bool = True):
        """æ¸…ç†æ—§è®°å¿†ï¼ˆæ™ºèƒ½æ¸…ç†ï¼‰"""
        cutoff = datetime.now() - timedelta(days=days_old)
        
        deleted = 0
        kept = 0
        
        for filepath in self.memory_dir.glob("*"):
            if filepath.suffix in ['.meta', '.json']:
                continue
                
            try:
                mtime = datetime.fromtimestamp(filepath.stat().st_mtime)
                
                # æ£€æŸ¥æ˜¯å¦é‡è¦
                is_important = False
                meta_file = filepath.with_suffix('.meta')
                if meta_file.exists() and keep_important:
                    with open(meta_file, 'r', encoding='utf-8') as f:
                        meta_info = json.load(f)
                        if meta_info.get("importance", 0) > 0.8:  # é«˜é‡è¦æ€§è®°å¿†
                            is_important = True
                
                if mtime < cutoff and not is_important:
                    filepath.unlink()
                    
                    # åˆ é™¤å…ƒæ–‡ä»¶
                    if meta_file.exists():
                        meta_file.unlink()
                    
                    deleted += 1
                    
                    # ä»ç¼“å­˜ä¸­ç§»é™¤
                    for key in list(self.memory_cache.keys()):
                        if self.memory_cache[key].get("filepath") == filepath:
                            del self.memory_cache[key]
                            
                    # ä»æŸåæ–‡ä»¶åˆ—è¡¨ä¸­ç§»é™¤
                    if str(filepath) in self.stats["corrupted_files"]:
                        self.stats["corrupted_files"].remove(str(filepath))
                else:
                    kept += 1
            except:
                continue
        
        print(f"è®°å¿†æ¸…ç†: åˆ é™¤äº† {deleted} ä¸ªæ—§è®°å¿†ï¼Œä¿ç•™äº† {kept} ä¸ªè®°å¿†")
        return deleted
    
    def manage_cache(self):
        """æ™ºèƒ½ç¼“å­˜ç®¡ç†"""
        if len(self.memory_cache) <= self.cache_size_limit:
            return
        
        # æŒ‰è®¿é—®é¢‘ç‡å’Œé‡è¦æ€§è®¡ç®—åˆ†æ•°ï¼Œæ·˜æ±°ä½åˆ†é¡¹ç›®
        cache_scores = []
        for key, entry in self.memory_cache.items():
            # åˆ†æ•° = è®¿é—®é¢‘ç‡ * 0.4 + é‡è¦æ€§ * 0.4 + æ—¶é—´è¡°å‡ * 0.2
            access_count = self.access_counter.get(key, 0)
            importance = entry.get("importance", 0.5)
            
            # æ—¶é—´è¡°å‡ï¼ˆæœ€è¿‘è®¿é—®çš„åˆ†æ•°é«˜ï¼‰
            time_since_access = (datetime.now() - entry.get("accessed", datetime.now())).total_seconds()
            time_score = max(0, 1 - time_since_access / (24 * 3600))  # 24å°æ—¶è¡°å‡
            
            score = access_count * 0.4 + importance * 0.4 + time_score * 0.2
            cache_scores.append((score, key))
        
        # æ’åºï¼Œä¿ç•™é«˜åˆ†é¡¹ç›®
        cache_scores.sort(reverse=True)
        keep_keys = {key for _, key in cache_scores[:self.cache_size_limit]}
        
        # ç§»é™¤ä½åˆ†é¡¹ç›®
        for key in list(self.memory_cache.keys()):
            if key not in keep_keys:
                del self.memory_cache[key]
    
    def consolidate_important_memories(self):
        """å·©å›ºé‡è¦è®°å¿†"""
        important_memories = []
        
        # æ”¶é›†é«˜é‡è¦æ€§è®°å¿†
        for filepath in self.memory_dir.glob("*"):
            if filepath.suffix in ['.meta', '.json']:
                continue
                
            # è·³è¿‡æŸåæ–‡ä»¶
            if str(filepath) in self.stats["corrupted_files"]:
                continue
                
            try:
                meta_file = filepath.with_suffix('.meta')
                if meta_file.exists():
                    with open(meta_file, 'r', encoding='utf-8') as f:
                        meta_info = json.load(f)
                        
                    if meta_info.get("importance", 0) > 0.7:  # ä¸­ç­‰ä»¥ä¸Šé‡è¦æ€§
                        important_memories.append((filepath, meta_info))
            except:
                continue
        
        # é‡æ–°å‹ç¼©é‡è¦è®°å¿†ï¼Œä½¿ç”¨æ›´å¯é çš„ç®—æ³•
        consolidated = 0
        for filepath, meta_info in important_memories:
            try:
                # è¯»å–åŸæ•°æ®
                with open(filepath, 'rb') as f:
                    compressed = f.read()
                
                algorithm = meta_info.get("algorithm", self.algorithm)
                memory_data = self.decompress_data(compressed, algorithm)
                
                if memory_data:
                    # é‡æ–°å‹ç¼©ä¸ºgzipï¼ˆæ›´å¯é ï¼‰
                    importance = meta_info.get("importance", 0.5)
                    recompressed, new_algorithm = self.compress_data(memory_data, importance)
                    
                    # å¦‚æœæ–°ç®—æ³•ä¸åŒï¼Œä¿å­˜æ–°æ–‡ä»¶
                    if new_algorithm != algorithm:
                        new_filename = filepath.stem.rsplit('_', 1)[0] + f"_{importance:.2f}{self.COMPRESSION_ALGORITHMS[new_algorithm]['extension']}"
                        new_filepath = filepath.parent / new_filename
                        
                        with open(new_filepath, 'wb') as f:
                            f.write(recompressed)
                        
                        # æ›´æ–°å…ƒæ•°æ®
                        meta_info["algorithm"] = new_algorithm
                        meta_info["consolidated_at"] = datetime.now().isoformat()
                        meta_info["original_algorithm"] = algorithm
                        
                        with open(new_filepath.with_suffix('.meta'), 'w', encoding='utf-8') as f:
                            json.dump(meta_info, f, ensure_ascii=False, indent=2)
                        
                        # åˆ é™¤æ—§æ–‡ä»¶
                        filepath.unlink()
                        if filepath.with_suffix('.meta').exists():
                            filepath.with_suffix('.meta').unlink()
                        
                        # æ›´æ–°ç¼“å­˜ä¸­çš„æ–‡ä»¶è·¯å¾„
                        for key, cache_entry in self.memory_cache.items():
                            if cache_entry.get("filepath") == filepath:
                                cache_entry["filepath"] = new_filepath
                                cache_entry["algorithm"] = new_algorithm
                        
                        consolidated += 1
            except Exception as e:
                print(f"å·©å›ºè®°å¿†å¤±è´¥ {filepath}: {e}")
        
        print(f"å·©å›ºäº† {consolidated} ä¸ªé‡è¦è®°å¿†")
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total_files = len([f for f in self.memory_dir.glob("*") if f.suffix not in ['.meta', '.json']])
        
        cache_hit_rate = 0
        total_accesses = self.stats["cache_hits"] + self.stats["cache_misses"]
        if total_accesses > 0:
            cache_hit_rate = self.stats["cache_hits"] / total_accesses
        
        avg_access_time = self.stats.get("avg_access_time", 0)
        if isinstance(avg_access_time, float):
            avg_access_time_ms = f"{avg_access_time:.2f}ms"
        else:
            avg_access_time_ms = f"{avg_access_time}ms"
        
        return {
            **self.stats,
            "cache_size": len(self.memory_cache),
            "total_files": total_files,
            "cache_hit_rate": f"{cache_hit_rate:.2%}",
            "avg_access_time": avg_access_time_ms,
            "algorithm": self.algorithm,
            "corrupted_files_count": len(self.stats["corrupted_files"]),
            "access_counter_size": len(self.access_counter),
            "zstd_available": ZSTD_AVAILABLE
        }

class IntelligentMemory:
    """æ™ºèƒ½è®°å¿†ç®¡ç†ç³»ç»Ÿï¼ˆå¢å¼ºç‰ˆï¼‰"""
    
    def __init__(self, config):
        self.config = config
        self.compressed_memory = CompressedMemory(config)
        
        # è®°å¿†åˆ†ç±»ï¼ˆå¢å¼ºï¼‰
        self.memory_categories = {
            "conversation": {"name": "å¯¹è¯è®°å¿†", "importance": 0.6},
            "knowledge": {"name": "çŸ¥è¯†è®°å¿†", "importance": 0.8},
            "preference": {"name": "åå¥½è®°å¿†", "importance": 0.7},
            "learning": {"name": "å­¦ä¹ è®°å¿†", "importance": 0.9},
            "system": {"name": "ç³»ç»Ÿè®°å¿†", "importance": 0.5},
            "concept": {"name": "æ¦‚å¿µè®°å¿†", "importance": 0.85},
            "fact": {"name": "äº‹å®è®°å¿†", "importance": 0.75},
            "experience": {"name": "ç»éªŒè®°å¿†", "importance": 0.7}
        }
        
        # çŸ­æœŸè®°å¿†ï¼ˆæœ€è¿‘å¯¹è¯ï¼‰- å¢å¼º
        self.short_term_memory = deque(maxlen=100)
        self.short_term_weights = {}
        
        # è®°å¿†ç´¢å¼•ï¼ˆå¢å¼ºï¼‰
        self.memory_index = defaultdict(list)
        self.concept_network = defaultdict(set)
        self.semantic_links = defaultdict(list)
        
        # è®°å¿†å·©å›ºé˜Ÿåˆ—
        self.consolidation_queue = deque(maxlen=50)
        
        # åˆå§‹åŒ–
        self.load_concept_network()
        print(f"âœ… è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def load_concept_network(self):
        """åŠ è½½æ¦‚å¿µç½‘ç»œ"""
        network_file = self.config.get_path("memory_dir") / "concept_network.json"
        if network_file.exists():
            try:
                with open(network_file, 'r', encoding='utf-8') as f:
                    network_data = json.load(f)
                    self.concept_network = defaultdict(set, {k: set(v) for k, v in network_data.get("concept_network", {}).items()})
                    self.semantic_links = defaultdict(list, network_data.get("semantic_links", {}))
                print(f"ğŸ“š åŠ è½½æ¦‚å¿µç½‘ç»œ: {len(self.concept_network)} ä¸ªæ¦‚å¿µ")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æ¦‚å¿µç½‘ç»œå¤±è´¥: {e}")
    
    def save_concept_network(self):
        """ä¿å­˜æ¦‚å¿µç½‘ç»œ"""
        network_file = self.config.get_path("memory_dir") / "concept_network.json"
        try:
            network_data = {
                "concept_network": {k: list(v) for k, v in self.concept_network.items()},
                "semantic_links": dict(self.semantic_links),
                "updated_at": datetime.now().isoformat()
            }
            with open(network_file, 'w', encoding='utf-8') as f:
                json.dump(network_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ¦‚å¿µç½‘ç»œå¤±è´¥: {e}")
    
    def remember(self, category: str, key: str, data: Any, 
                 metadata: Dict = None, tags: List[str] = None) -> bool:
        """è®°ä½ä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        # å‡†å¤‡å…ƒæ•°æ®
        full_metadata = {
            "category": category,
            "key": key,
            "tags": tags or [],
            "importance": metadata.get("importance", 
                self.memory_categories.get(category, {}).get("importance", 0.5)) if metadata else 
                self.memory_categories.get(category, {}).get("importance", 0.5),
            "context": metadata.get("context", "") if metadata else "",
            "related_concepts": [],
            **({} if metadata is None else metadata)
        }
        
        # æå–æ¦‚å¿µå¹¶å»ºç«‹ç½‘ç»œ
        if category == "knowledge" or category == "concept":
            concepts = self.extract_concepts(data, metadata)
            if concepts:
                full_metadata["concepts"] = concepts
                self.update_concept_network(key, concepts, data)
        
        # å»ºç«‹è¯­ä¹‰é“¾æ¥
        if tags:
            self.create_semantic_links(key, tags, category, data)
        
        # ä¿å­˜åˆ°å‹ç¼©å­˜å‚¨
        memory_id = f"{category}_{key}"
        success = self.compressed_memory.save_memory(memory_id, data, full_metadata)
        
        if success:
            # æ›´æ–°ç´¢å¼•
            self.memory_index[category].append(memory_id)
            
            # æ·»åŠ æ ‡ç­¾ç´¢å¼•
            if tags:
                for tag in tags:
                    self.memory_index[f"tag_{tag}"].append(memory_id)
            
            # å¦‚æœæ˜¯å¯¹è¯ï¼Œæ·»åŠ åˆ°çŸ­æœŸè®°å¿†
            if category == "conversation":
                self.short_term_memory.append({
                    "key": key,
                    "data": data,
                    "timestamp": datetime.now().isoformat(),
                    "importance": full_metadata["importance"]
                })
                # è®¾ç½®çŸ­æœŸè®°å¿†æƒé‡
                self.short_term_weights[key] = 1.0
            
            # æ·»åŠ åˆ°å·©å›ºé˜Ÿåˆ—
            if full_metadata["importance"] > 0.7:
                self.consolidation_queue.append(memory_id)
        
        return success
    
    def extract_concepts(self, data: Any, metadata: Dict) -> List[str]:
        """ä»æ•°æ®ä¸­æå–æ¦‚å¿µ"""
        concepts = []
        
        # ä»æ•°æ®ä¸­æå–
        if isinstance(data, dict):
            text_parts = []
            for value in data.values():
                if isinstance(value, str):
                    text_parts.append(value)
            text = " ".join(text_parts)
        elif isinstance(data, str):
            text = data
        else:
            text = str(data)
        
        # æå–ä¸­æ–‡æ¦‚å¿µï¼ˆ2-5ä¸ªå­—ï¼‰
        chinese_concepts = re.findall(r"[\u4e00-\u9fa5]{2,5}", text)
        concepts.extend(chinese_concepts[:5])
        
        # ä»æ ‡ç­¾ä¸­æå–
        if metadata and "tags" in metadata:
            for tag in metadata["tags"]:
                if isinstance(tag, str) and len(tag) >= 2:
                    concepts.append(tag)
        
        # å»é‡
        return list(set(concepts))
    
    def update_concept_network(self, key: str, concepts: List[str], data: Any):
        """æ›´æ–°æ¦‚å¿µç½‘ç»œ"""
        for concept in concepts:
            # æ·»åŠ æ¦‚å¿µåˆ°ç½‘ç»œ
            self.concept_network[concept].add(key)
            
            # å»ºç«‹æ¦‚å¿µä¹‹é—´çš„å…³è”
            for other_concept in concepts:
                if concept != other_concept:
                    self.concept_network[concept].add(other_concept)
        
        # å®šæœŸä¿å­˜ç½‘ç»œ
        if len(self.concept_network) % 10 == 0:
            self.save_concept_network()
    
    def create_semantic_links(self, key: str, tags: List[str], category: str, data: Any):
        """åˆ›å»ºè¯­ä¹‰é“¾æ¥"""
        # æŸ¥æ‰¾æœ‰ç›¸åŒæ ‡ç­¾çš„è®°å¿†
        for tag in tags:
            tag_key = f"tag_{tag}"
            if tag_key in self.memory_index:
                related_memories = self.memory_index[tag_key][-5:]
                if related_memories:
                    self.semantic_links[key].extend(related_memories)
    
    def recall(self, category: str = None, key: str = None, 
               tag: str = None, limit: int = 5, context: str = None, 
               sort_by: str = None, **kwargs) -> List[Any]:
        """å›å¿†ä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        memories = []
        
        if key and category:
            # ç²¾ç¡®å›å¿†
            memory_id = f"{category}_{key}"
            data = self.compressed_memory.load_memory(memory_id)
            if data:
                memories.append(data)
        
        elif tag:
            # æŒ‰æ ‡ç­¾å›å¿†ï¼ˆè€ƒè™‘ä¸Šä¸‹æ–‡ï¼‰
            tag_key = f"tag_{tag}"
            if tag_key in self.memory_index:
                memory_ids = self.memory_index[tag_key]
                
                # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ï¼Œè¿›è¡Œæ’åº
                if context:
                    scored_memories = []
                    for memory_id in memory_ids:
                        data = self.compressed_memory.load_memory(memory_id)
                        if data:
                            # è®¡ç®—ä¸ä¸Šä¸‹æ–‡çš„ç›¸ä¼¼åº¦
                            if isinstance(data, dict):
                                data_str = str(data)
                            else:
                                data_str = str(data)
                            
                            similarity = self.calculate_similarity(context, data_str)
                            scored_memories.append((similarity, data))
                    
                    # æŒ‰ç›¸ä¼¼åº¦æ’åº
                    scored_memories.sort(reverse=True)
                    memories = [data for _, data in scored_memories[:limit]]
                else:
                    for memory_id in memory_ids[:limit]:
                        data = self.compressed_memory.load_memory(memory_id)
                        if data:
                            memories.append(data)
        
        elif category:
            # æŒ‰åˆ†ç±»å›å¿†ï¼ˆè€ƒè™‘é‡è¦æ€§ï¼‰
            if category in self.memory_index:
                memory_ids = self.memory_index[category]
                
                # åŠ è½½å¹¶è¯„åˆ†
                scored_memories = []
                for memory_id in memory_ids:
                    data = self.compressed_memory.load_memory(memory_id)
                    if data:
                        # ä»ç¼“å­˜è·å–é‡è¦æ€§
                        cache_entry = self.compressed_memory.memory_cache.get(memory_id.split('_', 1)[1])
                        importance = cache_entry.get("importance", 0.5) if cache_entry else 0.5
                        scored_memories.append((importance, data))
                
                # æŒ‰é‡è¦æ€§æ’åº
                scored_memories.sort(reverse=True)
                memories = [data for _, data in scored_memories[:limit]]
        
        else:
            # è·å–çŸ­æœŸè®°å¿†ï¼ˆåŠ æƒï¼‰
            short_term = list(self.short_term_memory)
            if short_term:
                # æ ¹æ®æ’åºæ–¹å¼å¤„ç†
                if sort_by == "recent":
                    short_term.sort(key=lambda x: x.get("timestamp", ""), reverse=True)
                elif sort_by == "important":
                    short_term.sort(key=lambda x: x.get("importance", 0.5), reverse=True)
                
                # æŒ‰æ—¶é—´å’Œæƒé‡æ’åº
                weighted_memories = []
                for memory in short_term:
                    key_val = memory.get("key", "")
                    weight = self.short_term_weights.get(key_val, 1.0)
                    timestamp = memory.get("timestamp", "")
                    
                    # æ—¶é—´è¡°å‡
                    if timestamp:
                        try:
                            mem_time = datetime.fromisoformat(timestamp)
                            time_diff = (datetime.now() - mem_time).total_seconds()
                            time_factor = max(0, 1 - time_diff / (3600 * 24))
                        except:
                            time_factor = 0.5
                    else:
                        time_factor = 0.5
                    
                    score = weight * time_factor
                    weighted_memories.append((score, memory.get("data")))
                
                if not sort_by:
                    weighted_memories.sort(reverse=True)
                
                memories = [data for _, data in weighted_memories[:limit]]
        
        return memories
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆç®€å•å®ç°ï¼‰"""
        if not text1 or not text2:
            return 0.0
        
        text1_lower = text1.lower()
        text2_lower = text2.lower()
        
        # è®¡ç®—å…±åŒè¯æ±‡
        words1 = set(re.findall(r'\w+', text1_lower))
        words2 = set(re.findall(r'\w+', text2_lower))
        
        if not words1 or not words2:
            return 0.0
        
        common_words = words1.intersection(words2)
        
        # Jaccardç›¸ä¼¼åº¦
        similarity = len(common_words) / len(words1.union(words2))
        
        return similarity
    
    def search(self, query: str, category: str = None, limit: int = 10) -> List[Dict]:
        """æœç´¢è®°å¿†ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        # ä½¿ç”¨å‹ç¼©å†…å­˜çš„æ™ºèƒ½æœç´¢
        results = self.compressed_memory.search_memories(query, limit * 2)
        
        if category:
            results = [r for r in results 
                      if r["metadata"].get("category") == category]
        
        # æå–æ¦‚å¿µï¼ŒæŸ¥æ‰¾ç›¸å…³è®°å¿†
        if len(results) < limit:
            concepts = self.extract_concepts(query, {})
            for concept in concepts[:3]:
                if concept in self.concept_network:
                    related_keys = list(self.concept_network[concept])[:5]
                    for key in related_keys:
                        if '_' in key:
                            parts = key.split('_', 1)
                            if len(parts) == 2:
                                cat, mem_key = parts
                                data = self.compressed_memory.load_memory(key)
                                if data:
                                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                                    if not any(r["metadata"].get("key") == mem_key for r in results):
                                        results.append({
                                            "file": f"related_{key}",
                                            "data": data,
                                            "metadata": {"category": cat, "key": mem_key},
                                            "created": datetime.now().isoformat(),
                                            "relevance_score": 0.3,
                                            "source": "concept_network"
                                        })
        
        return results[:limit]
    
    def learn_from_conversation(self, user_input: str, ai_response: str, 
                               context: Dict = None):
        """ä»å¯¹è¯ä¸­å­¦ä¹ ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        # ç”Ÿæˆè®°å¿†é”®
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        memory_key = f"conv_{hashlib.md5(user_input.encode()).hexdigest()[:8]}_{timestamp}"
        
        # æå–å…³é”®è¯å’Œæ¦‚å¿µ
        keywords = self.extract_keywords(user_input + " " + ai_response)
        concepts = self.extract_concepts(user_input + " " + ai_response, {})
        
        # åˆ†æå¯¹è¯ç±»å‹
        conversation_type = self.analyze_conversation_type(user_input, ai_response)
        
        # ä¿å­˜å¯¹è¯
        conversation_data = {
            "user": user_input,
            "ai": ai_response,
            "context": context or {},
            "keywords": keywords,
            "concepts": concepts,
            "type": conversation_type,
            "length": len(user_input) + len(ai_response)
        }
        
        # æ ¹æ®å¯¹è¯ç±»å‹è®¾ç½®é‡è¦æ€§
        importance = 0.6
        if conversation_type == "knowledge_sharing":
            importance = 0.8
        elif conversation_type == "teaching":
            importance = 0.9
        elif conversation_type == "question_answer":
            importance = 0.7
        
        self.remember(
            category="conversation",
            key=memory_key,
            data=conversation_data,
            metadata={
                "importance": importance,
                "context": "å¯¹è¯å­¦ä¹ ",
                "emotion": self.detect_emotion(user_input),
                "conversation_type": conversation_type
            },
            tags=keywords + ["conversation"]
        )
        
        # å°è¯•æå–çŸ¥è¯†
        self.extract_knowledge(user_input, ai_response, keywords, concepts, conversation_type)
        
        # æ›´æ–°çŸ­æœŸè®°å¿†æƒé‡
        self.update_short_term_weights(memory_key, importance)
    
    def analyze_conversation_type(self, user_input: str, ai_response: str) -> str:
        """åˆ†æå¯¹è¯ç±»å‹"""
        user_lower = user_input.lower()
        ai_lower = ai_response.lower()
        
        teaching_patterns = ["æ˜¯", "å°±æ˜¯", "æŒ‡çš„æ˜¯", "æ„å‘³ç€", "å®šä¹‰ä¸º"]
        if any(pattern in user_lower for pattern in teaching_patterns):
            return "teaching"
        
        knowledge_indicators = ["çŸ¥è¯†", "ä¿¡æ¯", "æ•°æ®", "äº‹å®", "åŸç†"]
        if any(indicator in user_lower or indicator in ai_lower for indicator in knowledge_indicators):
            return "knowledge_sharing"
        
        question_indicators = ["å—", "ä»€ä¹ˆ", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å¦‚ä½•", "æ˜¯ä¸æ˜¯"]
        if any(indicator in user_lower for indicator in question_indicators):
            return "question_answer"
        
        chat_indicators = ["ä½ å¥½", "å†è§", "è°¢è°¢", "å“ˆå“ˆ", "å—¯"]
        if any(indicator in user_lower for indicator in chat_indicators):
            return "chat"
        
        return "general"
    
    def extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        words = re.findall(r'[\u4e00-\u9fa5]+|[a-zA-Z]+', text.lower())
        
        # ä¸­æ–‡åœç”¨è¯ï¼ˆå¢å¼ºç‰ˆï¼‰
        stop_words = {
            "çš„", "äº†", "å’Œ", "æ˜¯", "åœ¨", "æˆ‘", "æœ‰", "ä½ ", "ä»–", "å¥¹", 
            "å®ƒ", "è¿™", "é‚£", "å°±", "éƒ½", "ä¹Ÿ", "ä¸", "å—", "å‘¢", "å•Š",
            "å‘€", "å§", "å—¯", "å“¦", "å“ˆ", "å•¦", "å“‡", "å˜›", "å“Ÿ", "å“¼"
        }
        
        # è®¡ç®—è¯é¢‘
        word_freq = defaultdict(int)
        for word in words:
            if word not in stop_words and len(word) > 1:
                word_freq[word] += 1
        
        # æŒ‰é¢‘ç‡æ’åºï¼Œå–å‰10
        keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
        
        return [word for word, _ in keywords]
    
    def detect_emotion(self, text: str) -> str:
        """æ£€æµ‹æƒ…ç»ªï¼ˆå¢å¼ºç‰ˆï¼‰"""
        positive_words = {
            "å¥½", "å¼€å¿ƒ", "å¿«ä¹", "é«˜å…´", "è°¢è°¢", "å–œæ¬¢", "çˆ±", "æ£’", "å®Œç¾",
            "ä¼˜ç§€", "å‰å®³", "å¼ºå¤§", "ç¾å¥½", "å¹¸ç¦", "æ»¡æ„", "èµ", "æ£’æ£’", "å¤ªæ£’äº†"
        }
        negative_words = {
            "ä¸å¥½", "ç”Ÿæ°”", "éš¾è¿‡", "ä¼¤å¿ƒ", "è®¨åŒ", "æ¨", "ç³Ÿç³•", "å", "çƒ¦",
            "åƒåœ¾", "è®¨åŒ", "æ„¤æ€’", "å¤±æœ›", "æ‚²ä¼¤", "ç—›è‹¦", "éš¾å—", "å¯æ¶"
        }
        
        text_lower = text.lower()
        
        pos_count = sum(1 for word in positive_words if word in text_lower)
        neg_count = sum(1 for word in negative_words if word in text_lower)
        
        # è€ƒè™‘å¼ºåº¦
        strong_positive = {"çˆ±", "å¤ªæ£’äº†", "å®Œç¾", "ä¼˜ç§€"}
        strong_negative = {"æ¨", "åƒåœ¾", "å¯æ¶", "æ„¤æ€’"}
        
        for word in strong_positive:
            if word in text_lower:
                pos_count += 2
        
        for word in strong_negative:
            if word in text_lower:
                neg_count += 2
        
        if pos_count > neg_count * 2:
            return "very_positive"
        elif pos_count > neg_count:
            return "positive"
        elif neg_count > pos_count * 2:
            return "very_negative"
        elif neg_count > pos_count:
            return "negative"
        else:
            return "neutral"
    
    def extract_knowledge(self, question: str, answer: str, keywords: List[str], 
                          concepts: List[str], conversation_type: str):
        """æå–çŸ¥è¯†ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        # å¦‚æœå›ç­”åŒ…å«äº‹å®æ€§ä¿¡æ¯ï¼Œä¿å­˜ä¸ºçŸ¥è¯†
        factual_indicators = ["æ˜¯", "æœ‰", "å¯ä»¥", "èƒ½å¤Ÿ", "ä¼š", "è¦", "éœ€è¦", "å¿…é¡»", 
                             "åº”è¯¥", "ä¸€å®š", "é€šå¸¸", "ä¸€èˆ¬", "æ€»æ˜¯", "ä»ä¸"]
        
        has_factual = any(indicator in answer for indicator in factual_indicators)
        is_teaching = conversation_type == "teaching"
        
        if has_factual or is_teaching:
            # æå–å¯èƒ½çš„å®šä¹‰
            definition = None
            if "æ˜¯" in answer:
                parts = answer.split("æ˜¯", 1)
                if len(parts) == 2:
                    definition = parts[1].strip()
            
            knowledge_data = {
                "question": question,
                "answer": answer,
                "keywords": keywords,
                "concepts": concepts,
                "definition": definition,
                "type": "fact" if has_factual else "teaching",
                "conversation_type": conversation_type
            }
            
            # ç”ŸæˆçŸ¥è¯†é”®
            knowledge_key = hashlib.md5((question + answer).encode()).hexdigest()[:12]
            
            # è®¾ç½®é‡è¦æ€§
            importance = 0.8 if is_teaching else 0.7
            
            self.remember(
                category="knowledge",
                key=knowledge_key,
                data=knowledge_data,
                metadata={
                    "importance": importance,
                    "source": "conversation_extraction",
                    "extracted_from": f"conv_{hashlib.md5(question.encode()).hexdigest()[:8]}"
                },
                tags=keywords + ["knowledge", "extracted"]
            )
    
    def update_short_term_weights(self, memory_key: str, importance: float):
        """æ›´æ–°çŸ­æœŸè®°å¿†æƒé‡"""
        # æ–°è®°å¿†åˆå§‹æƒé‡
        self.short_term_weights[memory_key] = 1.0
        
        # é‡è¦è®°å¿†æƒé‡æ›´é«˜
        if importance > 0.7:
            self.short_term_weights[memory_key] = 2.0
        
        # è¡°å‡æ—§è®°å¿†æƒé‡
        for key in list(self.short_term_weights.keys()):
            if key != memory_key:
                self.short_term_weights[key] *= 0.9
                
                # ç§»é™¤æƒé‡è¿‡ä½çš„è®°å¿†
                if self.short_term_weights[key] < 0.1:
                    del self.short_term_weights[key]
    
    def get_short_term_memory(self, limit: int = 10) -> List[Dict]:
        """è·å–çŸ­æœŸè®°å¿†ï¼ˆåŠ æƒï¼‰"""
        memories = list(self.short_term_memory)
        
        if not memories:
            return []
        
        # æŒ‰æƒé‡æ’åº
        weighted_memories = []
        for memory in memories:
            key = memory.get("key", "")
            weight = self.short_term_weights.get(key, 0.5)
            weighted_memories.append((weight, memory))
        
        weighted_memories.sort(reverse=True)
        
        return [memory for _, memory in weighted_memories[:limit]]
    
    def consolidate_memories(self):
        """å·©å›ºè®°å¿†"""
        # å¤„ç†å·©å›ºé˜Ÿåˆ—
        while self.consolidation_queue:
            memory_id = self.consolidation_queue.popleft()
            
            # é‡æ–°åŠ è½½å’Œä¿å­˜é‡è¦è®°å¿†
            if '_' in memory_id:
                parts = memory_id.split('_', 1)
                if len(parts) == 2:
                    category, key = parts
                    data = self.compressed_memory.load_memory(memory_id)
                    if data:
                        # æé«˜é‡è¦æ€§
                        cache_entry = self.compressed_memory.memory_cache.get(key)
                        if cache_entry:
                            new_importance = min(1.0, cache_entry.get("importance", 0.5) + 0.1)
                            cache_entry["importance"] = new_importance
                            
                            # é‡æ–°ä¿å­˜
                            metadata = cache_entry.get("metadata", {})
                            metadata["importance"] = new_importance
                            metadata["consolidated"] = True
                            metadata["consolidated_at"] = datetime.now().isoformat()
                            
                            self.remember(category, key, data, metadata)
        
        # å‹ç¼©å†…å­˜çš„å·©å›º
        self.compressed_memory.consolidate_important_memories()
        
        # ä¿å­˜æ¦‚å¿µç½‘ç»œ
        self.save_concept_network()
    
    def cleanup(self, days_old: int = 30):
        """æ¸…ç†æ—§è®°å¿†"""
        deleted = self.compressed_memory.cleanup_old_memories(days_old, keep_important=True)
        
        # æ¸…ç†çŸ­æœŸè®°å¿†
        cutoff = datetime.now() - timedelta(days=min(7, days_old))
        old_count = 0
        
        for memory in list(self.short_term_memory):
            timestamp = memory.get("timestamp", "")
            if timestamp:
                try:
                    mem_time = datetime.fromisoformat(timestamp)
                    if mem_time < cutoff:
                        self.short_term_memory.remove(memory)
                        old_count += 1
                except:
                    pass
        
        print(f"æ¸…ç†äº† {old_count} æ¡æ—§çŸ­æœŸè®°å¿†")
        
        # æ‰§è¡Œå·©å›º
        self.consolidate_memories()
        
        return deleted + old_count
    
    def get_stats(self) -> Dict:
        """è·å–è®°å¿†ç»Ÿè®¡ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        memory_stats = self.compressed_memory.get_stats()
        
        # çŸ­æœŸè®°å¿†ç»Ÿè®¡
        short_term_by_type = defaultdict(int)
        for memory in self.short_term_memory:
            if isinstance(memory.get("data"), dict):
                conv_type = memory["data"].get("type", "unknown")
                short_term_by_type[conv_type] += 1
        
        return {
            **memory_stats,
            "categories": {cat: len(ids) for cat, ids in self.memory_index.items()},
            "short_term_count": len(self.short_term_memory),
            "short_term_by_type": dict(short_term_by_type),
            "total_memories": sum(len(ids) for ids in self.memory_index.values()),
            "concept_network_size": len(self.concept_network),
            "semantic_links_count": sum(len(links) for links in self.semantic_links.values()),
            "memory_categories": len(self.memory_categories)
        }

def main():
    """è®°å¿†ç³»ç»Ÿæµ‹è¯•"""
    print("æµ‹è¯•å¢å¼ºç‰ˆè®°å¿†ç³»ç»Ÿ...")
    
    # åˆ›å»ºé…ç½®
    from config import AIConfig
    config = AIConfig()
    
    # åˆ›å»ºè®°å¿†ç³»ç»Ÿ
    memory = IntelligentMemory(config)
    
    # æµ‹è¯•è®°å¿†
    test_data = {
        "name": "æµ‹è¯•ç”¨æˆ·",
        "preference": "å–œæ¬¢è“è‰²å’Œç»¿è‰²",
        "knowledge": "Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€",
        "last_visited": datetime.now().isoformat()
    }
    
    # ä¿å­˜è®°å¿†
    success = memory.remember(
        category="preference",
        key="user_prefs_v2",
        data=test_data,
        metadata={"importance": 0.9, "context": "ç”¨æˆ·åå¥½å’ŒçŸ¥è¯†"},
        tags=["user", "preference", "knowledge", "python"]
    )
    
    print(f"ä¿å­˜è®°å¿†: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
    
    # å›å¿†è®°å¿†
    recalled = memory.recall(category="preference", key="user_prefs_v2")
    print(f"å›å¿†è®°å¿†: {len(recalled)} æ¡")
    
    # æµ‹è¯•å¸¦æ’åºçš„å›å¿†
    recalled_sorted = memory.recall(limit=5, sort_by="recent")
    print(f"æŒ‰æ—¶é—´æ’åºå›å¿†: {len(recalled_sorted)} æ¡")
    
    # æœç´¢è®°å¿†
    search_results = memory.search("Python")
    print(f"æœç´¢ç»“æœ: {len(search_results)} æ¡")
    
    # è·å–ç»Ÿè®¡
    stats = memory.get_stats()
    print(f"è®°å¿†ç»Ÿè®¡:")
    for key, value in stats.items():
        if isinstance(value, dict):
            print(f"  {key}:")
            for k, v in value.items():
                print(f"    {k}: {v}")
        else:
            print(f"  {key}: {value}")
    
    # æµ‹è¯•å¯¹è¯å­¦ä¹ 
    memory.learn_from_conversation(
        "Pythonæ˜¯ä»€ä¹ˆï¼Ÿ",
        "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥ç®€æ´æ˜“è¯»è‘—ç§°ã€‚",
        {"topic": "ç¼–ç¨‹", "difficulty": "å…¥é—¨"}
    )
    
    print("\næµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main()